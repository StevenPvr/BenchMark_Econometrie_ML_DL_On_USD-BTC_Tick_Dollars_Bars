# Data Fetching Daemon - RÃ©cupÃ©ration Automatique de DonnÃ©es

Le daemon de rÃ©cupÃ©ration de donnÃ©es permet de collecter automatiquement les donnÃ©es BTC/USD en continu, sans intervention manuelle.

## ğŸš€ DÃ©marrage Rapide

### Lancer le daemon en arriÃ¨re-plan :
```bash
./scripts/run_data_fetching_daemon.sh start
```

### VÃ©rifier le statut :
```bash
./scripts/run_data_fetching_daemon.sh status
```

### Consulter les logs en temps rÃ©el :
```bash
./scripts/run_data_fetching_daemon.sh logs
```

### ArrÃªter le daemon :
```bash
./scripts/run_data_fetching_daemon.sh stop
```
# 2. Pour reprendre avec le fichier consolidÃ© comme base
```bash
./scripts/run_data_fetching_daemon.sh base

## ğŸ“‹ Commandes Disponibles

| Commande | Description |
|----------|-------------|
| `./scripts/run_data_fetching_daemon.sh start` | DÃ©marre le daemon en arriÃ¨re-plan |
| `./scripts/run_data_fetching_daemon.sh stop` | ArrÃªte le daemon proprement |
| `./scripts/run_data_fetching_daemon.sh restart` | RedÃ©marre le daemon |
| `./scripts/run_data_fetching_daemon.sh status` | Affiche le statut et les derniers logs |
| `./scripts/run_data_fetching_daemon.sh logs` | Suit les logs en temps rÃ©el |

## âš™ï¸ Fonctionnement

### Cycle Automatique :
1. **RÃ©cupÃ©ration** : TÃ©lÃ©charge 5 jours de donnÃ©es BTC/USD
2. **IncrÃ©mentation** : Avance automatiquement les dates (+5 jours)
3. **Pause** : Attend 5 minutes avant la prochaine itÃ©ration
4. **RÃ©cupÃ©ration d'erreurs** : Continue mÃªme en cas d'erreur

### Gestion des Dates :
- **DÃ©but** : Utilise les dates de `constants.py` au premier lancement
- **IncrÃ©mentation** : +5 jours automatiquement aprÃ¨s chaque succÃ¨s
- **Persistance** : Sauvegarde l'Ã©tat dans `data/fetch_dates_state.json`
- **Gestion des mois** : GÃ¨re automatiquement les fins de mois (28/30/31 jours)

### Gestion des Erreurs :
- **RÃ©cupÃ©ration automatique** : Continue aprÃ¨s les erreurs
- **Backoff exponentiel** : Attend plus longtemps aprÃ¨s les erreurs rÃ©pÃ©tÃ©es
- **ArrÃªt de sÃ©curitÃ©** : S'arrÃªte aprÃ¨s 5 erreurs consÃ©cutives
- **Logs dÃ©taillÃ©s** : Toutes les erreurs sont loggÃ©es

## ğŸ“ Fichiers CrÃ©Ã©s

```
data/
â”œâ”€â”€ fetch_dates_state.json          # Ã‰tat des dates actuelles
â”œâ”€â”€ raw/
â”‚   â””â”€â”€ dataset_raw.parquet/        # DonnÃ©es partitionnÃ©es
â”‚       â”œâ”€â”€ part-00000.parquet      # Tes donnÃ©es nettoyÃ©es
â”‚       â”œâ”€â”€ part-*.parquet          # Nouvelles donnÃ©es ajoutÃ©es
â”‚       â””â”€â”€ ...
â””â”€â”€ fetching_daemon.pid             # PID du daemon

logs/
â””â”€â”€ data_fetching_daemon.log        # Logs du daemon
```

## ğŸ”§ Configuration

### Modifier la frÃ©quence :
Ã‰diter `src/data_fetching/main.py` :
```python
AUTO_INCREMENT_DAYS: int = 5  # Jours par fenÃªtre
# Et dans main_loop():
delay_seconds: int = 300       # 5 minutes entre itÃ©rations
```

### Modifier le nombre de workers :
```python
max_workers: int = 6  # Workers parallÃ¨les (dÃ©faut: 6)
```

## ğŸ“Š Monitoring

### Logs en temps rÃ©el :
```bash
./scripts/run_data_fetching_daemon.sh logs
```

### VÃ©rifier les donnÃ©es collectÃ©es :
```bash
# Nombre de fichiers partitionnÃ©s
ls -la data/raw/dataset_raw.parquet/ | wc -l

# Taille totale
du -sh data/raw/dataset_raw.parquet/

# DerniÃ¨res donnÃ©es
python3 -c "
import pandas as pd
df = pd.read_parquet('data/raw/dataset_raw.parquet')
print(f'DonnÃ©es: {len(df):,} trades')
print(f'PÃ©riode: {df.timestamp.min()} â†’ {df.timestamp.max()}')
"
```

## ğŸ§  Pipeline OptimisÃ© MÃ©moire

Le systÃ¨me utilise une approche **zÃ©ro consolidation** pour Ã©conomiser drastiquement la RAM :

### Architecture Intelligente :
1. **Accumulation** : Fichiers parquet partitionnÃ©s restent sÃ©parÃ©s (pas de fusion)
2. **Traitement individuel** : `data_preparation` traite chaque fichier un par un
3. **RÃ©duction drastique** : Convertit **millions de trades â†’ milliers de dollar bars**
4. **Pas de fusion massive** : Ã‰vite de charger 72M lignes en mÃ©moire simultanÃ©ment

### Avantages MÃ©moire :
- âœ… **ZÃ©ro consolidation** des trades bruts (Ã©vite 72M lignes en RAM)
- âœ… **Traitement sÃ©quentiel** des fichiers (max 50M lignes Ã  la fois)
- âœ… **Compression finale** : Dollar bars = ~1/1000Ã¨me de la taille originale
- âœ… **ScalabilitÃ©** : Marche avec des datasets de plusieurs milliards de trades

### Comparaison :
```
âŒ Ancienne approche : Charger 72M trades â†’ 16GB RAM â†’ Consolidation
âœ… Nouvelle approche : 45M trades â†’ 6M trades â†’ 3M trades â†’ 3M trades â†’ Dollar bars
```

## ğŸ›‘ ArrÃªt d'Urgence

Si le daemon ne rÃ©pond plus :
```bash
# Trouver le PID
ps aux | grep "data_fetching"

# Tuer manuellement
kill -9 <PID>
rm -f data/fetching_daemon.pid
```

## ğŸ¯ Usage Typique

```bash
# Lancer pour collecter des donnÃ©es en continu
./scripts/run_data_fetching_daemon.sh start

# VÃ©rifier rÃ©guliÃ¨rement
./scripts/run_data_fetching_daemon.sh status

# Quand tu as assez de donnÃ©es
./scripts/run_data_fetching_daemon.sh stop
```

Le daemon tournera indÃ©finiment et collectera automatiquement de plus en plus de donnÃ©es historiques ! ğŸš€
